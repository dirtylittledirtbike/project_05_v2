{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime \n",
    "import re\n",
    "from urlextract import URLExtract\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = URLExtract()\n",
    "# remove all urls from posts\n",
    "def replace_urls(x):\n",
    "    urls = extractor.find_urls(x)\n",
    "    if urls:\n",
    "        x_new = replace_urls(x.replace(urls[0],''))\n",
    "        return x_new\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('depression_unclean.csv', low_memory=False)\n",
    "df_full['date'] = df_full.created_utc.apply(lambda x: pd.to_datetime(time.ctime(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full.drop_duplicates(subset=['id'], inplace=True)\n",
    "df_full['selftext'] = df_full.selftext.map(lambda x: re.sub('\\n',' ',str(x)))\n",
    "df_full['selftext'] = df_full.selftext.map(lambda x: re.sub('  ',' ',str(x)))\n",
    "df_full['selftext'] = df_full.selftext.map(lambda x: replace_urls(x))\n",
    "df_full['date'] = df_full.created_utc.apply(lambda x: pd.to_datetime(time.ctime(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='date,date'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make sure theres a relatively even distribution for all months in concatenated data\n",
    "\n",
    "df_full.groupby([df_full[\"date\"].dt.year, df_full[\"date\"].dt.month]).size().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['feel',\n",
    " 'want',\n",
    " 'myself',\n",
    " 'people', 'feelings', 'feeling', 'not', 'fuck',\n",
    " 'fucking','fucks', 'day', 'days', 'love', 'hate', 'help', 'try']\n",
    "#  'help',\n",
    "#  'depression',\n",
    "#  'nan',\n",
    "#  'fucking',\n",
    "#  'mental',\n",
    "#  'hate',\n",
    "#  'health',\n",
    "#  'life',\n",
    "#  'need',\n",
    "#  'anxiety',\n",
    "#  'talk',\n",
    "#  'anymore',\n",
    "#  'die',\n",
    "#  'depressed',\n",
    "#  'friends',\n",
    "#  'fuck',\n",
    "#  'feeling',\n",
    "# #  'kill',\n",
    "#  'time',\n",
    "#  'shit',\n",
    "#  'tired',\n",
    "#  'happy',\n",
    "#  'better',\n",
    "#  'things',\n",
    "#  'going',\n",
    "#  'bad',\n",
    "#  'way',\n",
    "#  'care',\n",
    "#  'good',\n",
    "#  'wish',\n",
    "#  'person',\n",
    "#  'sad',\n",
    "#  'day',\n",
    "#  'makes',\n",
    "#  'years',\n",
    "#  'right',\n",
    "#  'tell',\n",
    "#  'self',\n",
    "#  'love',\n",
    "#  'worse',\n",
    "#  'try',\n",
    "#  'thoughts',\n",
    "#  'live',\n",
    "#  'friend',\n",
    "#  'stop',\n",
    "#  'feel',\n",
    "#  'help',\n",
    "#  'nan',\n",
    "#  'people',\n",
    "#  'fucking',\n",
    "#  'hate',\n",
    "#  'myself',\n",
    "#  'mental',\n",
    "#  'health',\n",
    "#  'depressed',\n",
    "#  'fuck',\n",
    "#  'shit',\n",
    "#  'self',\n",
    "#  'person',\n",
    "#  'makes',\n",
    "#  'friends',\n",
    "#  'feeling',\n",
    "#  'better',\n",
    "#  'depression',\n",
    "#  'wanna',\n",
    "#  'sad',\n",
    "#  'good',\n",
    "#  'way',\n",
    "#  'try',\n",
    "#  'understand',\n",
    "#  'wrong',\n",
    "#  'lonely',\n",
    "#  'advice',\n",
    "# '&amp;#x200B;',\n",
    "#   'lose',\n",
    "#  'find',\n",
    "#  'care',\n",
    "#  'feelings',\n",
    "#  'idk',\n",
    "#  'emotions',\n",
    "#  'bad',\n",
    "#  'issues',\n",
    "#  'talking',\n",
    "#  'stupid',\n",
    "#  'actually',\n",
    "#  'ask',\n",
    "#  'like',\n",
    "#  'right',\n",
    "#  'thoughts',\n",
    "#  'guilty',\n",
    "#  'illness',\n",
    "#  'life',\n",
    "#  'therapy',\n",
    "#  'things',\n",
    "#  'social','best', 'bit', 'lot', 'great', 'says', 'getting', 'numb', 'come', 'amp', 'thing', 'little', 'today', 'started', 'maybe', 'feels', 'feel', 'feeling',\n",
    "# \"told\", 'tomorrow', 'everyday', 'future', 'reason', 'pay', 'got', 'said', 'everyday', 'tomorrow', 'week', 'old', 'start', 'anxious', 'able', 'tried', 'features']+ ['think', 'thinking', 'happen', 'look', 'not', 'have', 'will', 'ill', 'sorry', 'alot', 'point', 'cuz', 'kinda', 'tell', 'exist', 'wait','post', 'have'] + ['let','happen','think','word','kinda','say','matt']\n",
    "              \n",
    "              \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/Users/collinswestnedge/programming/Metis_Online/project_04/pickles/'\n",
    "\n",
    "# with open(path+'stop_words_test.pickle', 'wb') as file:\n",
    "#     pickle.dump(set(stop_words), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path+'stop_words_test.pickle', 'rb') as file:\n",
    "#     stop_words = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for topic modeling... for speed going to pass all documents\n",
    "# to be cleaned with spacy and treat it as a preprocessing step (following their documentation)\n",
    "\n",
    "class NLPPipe:\n",
    "    \n",
    "    def __init__(self, vectorizer, tokenizer, disable, pos, nlp, lemma=False):\n",
    "        \n",
    "        self.vectorizer = vectorizer\n",
    "        self.tokenize = tokenizer\n",
    "        self.disable = disable\n",
    "        self.pos_list = pos\n",
    "        self.lemma = lemma\n",
    "        self.nlp = nlp\n",
    "        self.stop_words = stop_words\n",
    "    \n",
    "    def process_text(self, text):\n",
    "\n",
    "#         nlp = spacy.load(\"en\")\n",
    "#         nlp.vocab[\" \"].is_stop = True\n",
    "#         nlp.vocab[\"like\"].is_stop = True\n",
    "#         nlp.vocab[\"think\"].is_stop = True\n",
    "#         nlp.vocab[\"know\"].is_stop = True\n",
    "#         nlp.Defaults.stop_words |= set(self.stop_words)\n",
    "        \n",
    "#         for word in nlp.Defaults.stop_words:\n",
    "#             lex = nlp.vocab[word]\n",
    "#             lex.is_stop = True\n",
    "        my_stop_words = self.stop_words + list(self.nlp.Defaults.stop_words)\n",
    "        text_full = [] \n",
    "        for doc in self.nlp.pipe(text, disable=self.disable):\n",
    "            # if part of speech list isnt empty return matches for pos\n",
    "            if self.pos_list:\n",
    "                tokens = [(ent.text) for ent in doc if not ent.is_stop and not ent.is_punct and ent.pos_ in self.pos_list]\n",
    "                cleaned_text = \" \".join(tokens)\n",
    "                text_full.append(cleaned_text)\n",
    "            elif self.lemma == True:\n",
    "                tokens = [(ent.lemma_) for ent in doc if not ent.is_stop and not ent.is_punct]\n",
    "                cleaned_text = \" \".join(tokens)\n",
    "                text_full.append(cleaned_text)\n",
    "            else:\n",
    "                tokens = [(ent.text) for ent in doc if ent.text not in my_stop_words and not ent.is_punct]\n",
    "                cleaned_text = \" \".join(tokens)\n",
    "                text_full.append(cleaned_text)\n",
    "\n",
    "        return text_full\n",
    "    \n",
    "    def fit(self, text):\n",
    "        clean_text = self.process_text(text)\n",
    "        return self.vectorizer.fit(clean_text)\n",
    "    \n",
    "    def transform(self, text):\n",
    "        clean_text = self.process_text(text)\n",
    "        return self.vectorizer.transform(clean_text)\n",
    "    \n",
    "    def fit_transform(self, text):\n",
    "        clean_text = self.process_text(text)\n",
    "        return self.vectorizer.fit_transform(clean_text)\n",
    "    \n",
    "    def display_topics(self, model, feature_names, no_top_words, topic_names=None):\n",
    "        for ix, topic in enumerate(model.components_):\n",
    "            if not topic_names or not topic_names[ix]:\n",
    "                print(\"\\nTopic \", ix)\n",
    "            else:\n",
    "                print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "            print(\", \".join([feature_names[i]\n",
    "                            for i in topic.argsort()[:-no_top_words - 1:-1]])) \n",
    "    \n",
    "    def fit_transform_nmf(self, nmf, word_vec, feature_names, n_words):\n",
    "        '''\n",
    "        Description:\n",
    "        fit and display top words from nmf\n",
    "        on vectorized words\n",
    "        '''\n",
    "        doc_topic = nmf.fit_transform(word_vect)\n",
    "        self.display_topics(nmf, feature_names, n_words)\n",
    "        return doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = df_full_clean[['title','selftext']].sample(1)\n",
    "# corpus.title.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en\")\n",
    "\n",
    "nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['per',\n",
       " 'next',\n",
       " 'often',\n",
       " 'third',\n",
       " 'into',\n",
       " 'down',\n",
       " 'thence',\n",
       " 'hence',\n",
       " 'almost',\n",
       " 'or',\n",
       " 'her',\n",
       " 'fifty',\n",
       " 'full',\n",
       " 'most',\n",
       " 'anything',\n",
       " 'forty',\n",
       " 'everyone',\n",
       " 'during',\n",
       " 'you',\n",
       " 'be',\n",
       " 'did',\n",
       " 'which',\n",
       " 'how',\n",
       " '’ll',\n",
       " 'these',\n",
       " 'sixty',\n",
       " 'very',\n",
       " \"'d\",\n",
       " 'no',\n",
       " 'whose',\n",
       " 'throughout',\n",
       " 'its',\n",
       " 'used',\n",
       " 'he',\n",
       " '‘s',\n",
       " 'not',\n",
       " 'n‘t',\n",
       " 'thereafter',\n",
       " 'meanwhile',\n",
       " 'within',\n",
       " 'whence',\n",
       " 'this',\n",
       " 'whereby',\n",
       " 'yours',\n",
       " 'please',\n",
       " 'up',\n",
       " 'themselves',\n",
       " 'any',\n",
       " 'and',\n",
       " 'beside',\n",
       " 'whereas',\n",
       " 'seeming',\n",
       " 'really',\n",
       " 'again',\n",
       " 'become',\n",
       " 'but',\n",
       " 'go',\n",
       " 'part',\n",
       " 'his',\n",
       " 'via',\n",
       " 'anyone',\n",
       " 'unless',\n",
       " 'alone',\n",
       " 'onto',\n",
       " 'becomes',\n",
       " 'the',\n",
       " 'why',\n",
       " 'out',\n",
       " 'were',\n",
       " 'fifteen',\n",
       " 'hereby',\n",
       " 'after',\n",
       " 'on',\n",
       " 'quite',\n",
       " 'when',\n",
       " 'nine',\n",
       " 'latter',\n",
       " 'moreover',\n",
       " 'cannot',\n",
       " 'sometimes',\n",
       " 'same',\n",
       " 'amount',\n",
       " 'move',\n",
       " 'with',\n",
       " 'thus',\n",
       " 'our',\n",
       " \"'m\",\n",
       " 'us',\n",
       " 'my',\n",
       " 'at',\n",
       " 'whenever',\n",
       " 'all',\n",
       " '’s',\n",
       " 'whither',\n",
       " 'mine',\n",
       " 'through',\n",
       " 'own',\n",
       " 'who',\n",
       " 'below',\n",
       " 'behind',\n",
       " 'seemed',\n",
       " 'as',\n",
       " 'yourself',\n",
       " 'formerly',\n",
       " 'somewhere',\n",
       " 'himself',\n",
       " 'by',\n",
       " 'now',\n",
       " 'nor',\n",
       " 'everything',\n",
       " 'towards',\n",
       " 'whatever',\n",
       " 'do',\n",
       " 'anywhere',\n",
       " 'latterly',\n",
       " 'hereupon',\n",
       " 'some',\n",
       " 'more',\n",
       " 'herself',\n",
       " \"n't\",\n",
       " 'along',\n",
       " 'see',\n",
       " 'made',\n",
       " 'otherwise',\n",
       " 'whereupon',\n",
       " 'though',\n",
       " 'could',\n",
       " 'are',\n",
       " 'afterwards',\n",
       " 'less',\n",
       " 'put',\n",
       " 'thru',\n",
       " 'four',\n",
       " 'where',\n",
       " 'me',\n",
       " '‘d',\n",
       " 'must',\n",
       " 'few',\n",
       " 'ever',\n",
       " 'somehow',\n",
       " 'still',\n",
       " 'here',\n",
       " 'does',\n",
       " 'already',\n",
       " 'never',\n",
       " 'rather',\n",
       " 'indeed',\n",
       " 'therefore',\n",
       " 'both',\n",
       " 'someone',\n",
       " 'thereby',\n",
       " 'of',\n",
       " 'either',\n",
       " 'that',\n",
       " 'will',\n",
       " 'seem',\n",
       " 'itself',\n",
       " 'is',\n",
       " 'herein',\n",
       " 'amongst',\n",
       " 'until',\n",
       " 'without',\n",
       " 'in',\n",
       " 'hers',\n",
       " 'about',\n",
       " 'two',\n",
       " 'whom',\n",
       " 'every',\n",
       " 'to',\n",
       " 'sometime',\n",
       " 'whoever',\n",
       " 'ten',\n",
       " 'their',\n",
       " 'further',\n",
       " 'what',\n",
       " 'bottom',\n",
       " 'so',\n",
       " 'can',\n",
       " 'regarding',\n",
       " '’m',\n",
       " 'something',\n",
       " \"'ve\",\n",
       " '‘ll',\n",
       " 'top',\n",
       " 'has',\n",
       " 'say',\n",
       " 'we',\n",
       " 'for',\n",
       " 'a',\n",
       " 'make',\n",
       " 'seems',\n",
       " 'only',\n",
       " 'there',\n",
       " 'three',\n",
       " 'whole',\n",
       " 'your',\n",
       " 'if',\n",
       " 'twelve',\n",
       " 'everywhere',\n",
       " 'ours',\n",
       " 'namely',\n",
       " 'another',\n",
       " 'show',\n",
       " 'thereupon',\n",
       " 'whether',\n",
       " 'beyond',\n",
       " 'becoming',\n",
       " 'front',\n",
       " 'before',\n",
       " 'however',\n",
       " 'wherever',\n",
       " 'none',\n",
       " 'also',\n",
       " 'from',\n",
       " 'elsewhere',\n",
       " '‘ve',\n",
       " '’re',\n",
       " 'eleven',\n",
       " 'yourselves',\n",
       " 'anyway',\n",
       " 'had',\n",
       " 'serious',\n",
       " 'nowhere',\n",
       " 'beforehand',\n",
       " 'one',\n",
       " '‘re',\n",
       " 'each',\n",
       " 'him',\n",
       " 'against',\n",
       " 'being',\n",
       " 'nothing',\n",
       " 'once',\n",
       " 'might',\n",
       " 'above',\n",
       " 'other',\n",
       " 'such',\n",
       " 'she',\n",
       " \"'re\",\n",
       " 'between',\n",
       " 'yet',\n",
       " 'call',\n",
       " 'because',\n",
       " 'noone',\n",
       " 'under',\n",
       " 'since',\n",
       " 'twenty',\n",
       " 'been',\n",
       " 'except',\n",
       " 'using',\n",
       " 'together',\n",
       " 'several',\n",
       " 'anyhow',\n",
       " 'would',\n",
       " 'among',\n",
       " 'done',\n",
       " 'get',\n",
       " 'neither',\n",
       " 'they',\n",
       " 'due',\n",
       " 'nevertheless',\n",
       " 'off',\n",
       " \"'ll\",\n",
       " '’ve',\n",
       " 'even',\n",
       " 'whereafter',\n",
       " 'eight',\n",
       " 'first',\n",
       " 'have',\n",
       " 'just',\n",
       " 'doing',\n",
       " 'toward',\n",
       " 'i',\n",
       " 'upon',\n",
       " 'give',\n",
       " 'those',\n",
       " \"'s\",\n",
       " 'them',\n",
       " 'although',\n",
       " 'least',\n",
       " 'perhaps',\n",
       " 'name',\n",
       " 'myself',\n",
       " 'wherein',\n",
       " 'much',\n",
       " 'an',\n",
       " 'former',\n",
       " 'am',\n",
       " 'besides',\n",
       " 'last',\n",
       " 'hundred',\n",
       " 'therein',\n",
       " 'empty',\n",
       " '‘m',\n",
       " 'nobody',\n",
       " 'ca',\n",
       " 'five',\n",
       " 'mostly',\n",
       " 'was',\n",
       " 'over',\n",
       " 'well',\n",
       " 'then',\n",
       " 'hereafter',\n",
       " 'while',\n",
       " 'than',\n",
       " 'else',\n",
       " 'always',\n",
       " 'various',\n",
       " 'too',\n",
       " 'six',\n",
       " 'may',\n",
       " 'enough',\n",
       " 'keep',\n",
       " 'around',\n",
       " 'others',\n",
       " 'side',\n",
       " 'across',\n",
       " 'became',\n",
       " 'many',\n",
       " 're',\n",
       " 'ourselves',\n",
       " 'should',\n",
       " 'take',\n",
       " 'n’t',\n",
       " '’d',\n",
       " 'back',\n",
       " 'it']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nlp.vocab['dood'].is_stop = True\n",
    "list(nlp.Defaults.stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wut cool today', 'HATE melatonin', 'today great']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is for debugging the text preprocessing step comment out later...\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "today = ['wut', 'cool', 'heyhey']\n",
    "def process_text(nlp, text, pos_list, lemma=False):\n",
    "    today = stop_words + list(nlp.Defaults.stop_words)\n",
    "\n",
    "#     nlp = spacy.load(\"en\")\n",
    "# #     nlp.vocab[\"myself\"].is_stop = False\n",
    "#     nlp.vocab[\" \"].is_stop = True\n",
    "# #     nlp.vocab[\"like\"].is_stop = True\n",
    "# #     nlp.vocab[\"think\"].is_stop = True\n",
    "# #     nlp.vocab[\"know\"].is_stop = True\n",
    "#     nlp.Defaults.stop_words |= set(stop_words)\n",
    "#     print(nlp.vocab['love'].is_stop)\n",
    "#     test_words['dood']\n",
    "#     for word in test_word:\n",
    "#         lex = nlp.vocab[word]\n",
    "#         lex.is_stop = True\n",
    "#     nlp.vocab['dood'].is_stop = True\n",
    "\n",
    "    text_full = [] \n",
    "    for doc in nlp.pipe(text, disable=['parser', 'ner']):\n",
    "        # if part of speech list isnt empty return matches for pos\n",
    "        if pos_list:\n",
    "            tokens = [(ent.pos_) for ent in doc if not ent.is_stop and not ent.is_punct and ent.pos_ in pos_list]\n",
    "            cleaned_text = \" \".join(tokens)\n",
    "            text_full.append(cleaned_text)\n",
    "        elif lemma == True:\n",
    "            tokens = [(ent.lemma_) for ent in doc if not ent.is_stop and not ent.is_punct]\n",
    "            cleaned_text = \" \".join(tokens)\n",
    "            text_full.append(cleaned_text)\n",
    "        else:\n",
    "            tokens = [(ent.text) for ent in doc if ent.text not in today and not ent.is_punct]\n",
    "            cleaned_text = \" \".join(tokens)\n",
    "            text_full.append(cleaned_text)\n",
    "            \n",
    "    return text_full\n",
    "\n",
    "# print(corpus.title.values[0])\n",
    "# print()\n",
    "\n",
    "corp = ['wut cool today', 'HATE hate you melatonin whatever', 'today is great']\n",
    "\n",
    "a = process_text(nlp, corp, pos_list=[], lemma=False)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en\")\n",
    "for word in stop_words:\n",
    "    nlp.vocab[word].is_stop = True\n",
    "    lex.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.dropna(subset=['selftext'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding title of post and post as one column to get a little more information for topic modeling\n",
    "\n",
    "df_full['text_title'] = df_full.title + ' ' + df_full.selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full_clean.to_csv('/Users/collinswestnedge/programming/Metis_Online/project_04/data/data_full_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.title.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_null = df_full[(df_full.selftext != '[removed]')&(df_full.selftext != '[deleted]') & (df_full.selftext != 'nan')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_null.drop_duplicates(subset=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-9271ffa98718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                   )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mword_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-e02df4675b6f>\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mclean_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-e02df4675b6f>\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mtext_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_stop_words\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_punct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mcleaned_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mtext_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-e02df4675b6f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mtext_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_stop_words\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_punct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mcleaned_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mtext_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# corpus = df_full_clean.selftext\n",
    "corpus = df_non_null.text_title\n",
    "\n",
    "tfdif_vect = TfidfVectorizer(\n",
    "                             strip_accents='unicode',\n",
    "                             min_df = 10,\n",
    "                             ngram_range=(1, 1),\n",
    "                             max_df = .5, \n",
    "                             token_pattern = r'\\b[a-zA-Z]{3,}\\b'\n",
    "                             )\n",
    "\n",
    "pipeline = NLPPipe(vectorizer=tfdif_vect,\n",
    "                  tokenizer=None,\n",
    "                  disable=[\"tagger\",\"parser\", \"ner\"],\n",
    "                  pos=[],\n",
    "                  nlp=nlp,\n",
    "                  stop_words=stop_words,\n",
    "                  lemma=False,\n",
    "                  )\n",
    "\n",
    "word_vect = pipeline.fit_transform(corpus).toarray()\n",
    "features = pipeline.vectorizer.get_feature_names()\n",
    "\n",
    "# nmf = NMF(10, alpha=.1, l1_ratio=.5, random_state=0, max_iter=700, init='nndsvd')\n",
    "# doc_topic = pipeline.fit_transform_nmf(nmf, word_vect, features, n_words=100)\n",
    "# term_topic = nmf.components_\n",
    "\n",
    "# reg_nmf = NMF(11, alpha=.05, l1_ratio=.5, random_state=0, max_iter=700)\n",
    "# reg_doc_topic = pipeline.fit_transform_nmf(nmf, word_vect, features, n_words=100)\n",
    "# reg_topic_term = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abd</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abilify</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>abit</th>\n",
       "      <th>...</th>\n",
       "      <th>ziemlich</th>\n",
       "      <th>zip</th>\n",
       "      <th>zoloft</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoned</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58020</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58021</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58022</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58023</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58024</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58025 rows × 11920 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abandon  abandoned  abandoning  abandonment  abd  abdomen  abilify  \\\n",
       "0          0.0        0.0         0.0          0.0  0.0      0.0      0.0   \n",
       "1          0.0        0.0         0.0          0.0  0.0      0.0      0.0   \n",
       "2          0.0        0.0         0.0          0.0  0.0      0.0      0.0   \n",
       "3          0.0        0.0         0.0          0.0  0.0      0.0      0.0   \n",
       "4          0.0        0.0         0.0          0.0  0.0      0.0      0.0   \n",
       "...        ...        ...         ...          ...  ...      ...      ...   \n",
       "58020      0.0        0.0         0.0          0.0  0.0      0.0      0.0   \n",
       "58021      0.0        0.0         0.0          0.0  0.0      0.0      0.0   \n",
       "58022      0.0        0.0         0.0          0.0  0.0      0.0      0.0   \n",
       "58023      0.0        0.0         0.0          0.0  0.0      0.0      0.0   \n",
       "58024      0.0        0.0         0.0          0.0  0.0      0.0      0.0   \n",
       "\n",
       "       abilities  ability  abit  ...  ziemlich  zip  zoloft  zombie  zone  \\\n",
       "0            0.0      0.0   0.0  ...       0.0  0.0     0.0     0.0   0.0   \n",
       "1            0.0      0.0   0.0  ...       0.0  0.0     0.0     0.0   0.0   \n",
       "2            0.0      0.0   0.0  ...       0.0  0.0     0.0     0.0   0.0   \n",
       "3            0.0      0.0   0.0  ...       0.0  0.0     0.0     0.0   0.0   \n",
       "4            0.0      0.0   0.0  ...       0.0  0.0     0.0     0.0   0.0   \n",
       "...          ...      ...   ...  ...       ...  ...     ...     ...   ...   \n",
       "58020        0.0      0.0   0.0  ...       0.0  0.0     0.0     0.0   0.0   \n",
       "58021        0.0      0.0   0.0  ...       0.0  0.0     0.0     0.0   0.0   \n",
       "58022        0.0      0.0   0.0  ...       0.0  0.0     0.0     0.0   0.0   \n",
       "58023        0.0      0.0   0.0  ...       0.0  0.0     0.0     0.0   0.0   \n",
       "58024        0.0      0.0   0.0  ...       0.0  0.0     0.0     0.0   0.0   \n",
       "\n",
       "       zoned  zones  zoning  zoo  zoom  \n",
       "0        0.0    0.0     0.0  0.0   0.0  \n",
       "1        0.0    0.0     0.0  0.0   0.0  \n",
       "2        0.0    0.0     0.0  0.0   0.0  \n",
       "3        0.0    0.0     0.0  0.0   0.0  \n",
       "4        0.0    0.0     0.0  0.0   0.0  \n",
       "...      ...    ...     ...  ...   ...  \n",
       "58020    0.0    0.0     0.0  0.0   0.0  \n",
       "58021    0.0    0.0     0.0  0.0   0.0  \n",
       "58022    0.0    0.0     0.0  0.0   0.0  \n",
       "58023    0.0    0.0     0.0  0.0   0.0  \n",
       "58024    0.0    0.0     0.0  0.0   0.0  \n",
       "\n",
       "[58025 rows x 11920 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(word_vect)\n",
    "df.columns = pipeline.vectorizer.get_feature_names()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "        ... \n",
       "58020    0.0\n",
       "58021    0.0\n",
       "58022    0.0\n",
       "58023    0.0\n",
       "58024    0.0\n",
       "Name: fuck, Length: 58025, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fuck']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(10, alpha=.1, l1_ratio=.5, random_state=0, max_iter=700, init='nndsvd')\n",
    "doc_topic = pipeline.fit_transform_nmf(nmf, word_vect, features, n_words=100)\n",
    "term_topic = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        print(\"\\nTopic: \", ix)\n",
    "        print([round(topic[i], 4) for i in topic.argsort()[:-no_top_words - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf, pipeline.vectorizer.get_feature_names(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(nmf.components_).T\n",
    "test.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
